import json
import re
import argparse
import os
import time
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from enum import Enum


class NodeStatus(Enum):
    """èŠ‚ç‚¹çŠ¶æ€æšä¸¾"""
    CORRECT = "correct"           # æ­£ç¡®èŠ‚ç‚¹
    SELF_ERROR = "self_error"     # èŠ‚ç‚¹æœ¬èº«é”™è¯¯
    PARENT_ERROR = "parent_error" # çˆ¶èŠ‚ç‚¹é”™è¯¯å¯¼è‡´çš„é”™è¯¯


@dataclass
class NodeRecord:
    """èŠ‚ç‚¹è®°å½•æ•°æ®ç±»"""
    statement: str
    positions: List[int]          # å‡ºç°çš„ä½ç½®åˆ—è¡¨
    status_history: List[NodeStatus]  # æ¯æ¬¡å‡ºç°æ—¶çš„çŠ¶æ€
    occurrence_count: int = 0

@dataclass
class StatementNode:
    """StatementèŠ‚ç‚¹ç±» - ç”¨äºåå¤„ç†"""
    original_statement: str       # åŸå§‹è¯­å¥
    input_entity: str            # è¾“å…¥å®ä½“
    output_entity: str           # è¾“å‡ºå®ä½“
    output_parsed: Dict[str, Any]  # è§£æåçš„è¾“å‡ºç»“æ„
    occurrence_count: int = 0    # å‡ºç°æ¬¡æ•°
    is_correct: bool = False     # èŠ‚ç‚¹æœ¬èº«æ˜¯å¦æ­£ç¡®ï¼ˆå¯ä»¥ä»å‰ææ¨å¯¼å‡ºæ¥ï¼‰
    is_premise: bool = False     # æ˜¯å¦ä¸ºå‰ææ¡ä»¶
    node_type: str = "unknown"   # èŠ‚ç‚¹ç±»å‹ (premise/derived/hallucination)
    first_occurrence_index: int = -1  # é¦–æ¬¡å‡ºç°çš„ç´¢å¼•
    
    # æ–°å¢ï¼šæ¨ç†è·¯å¾„å®Œæ•´æ€§è¿½è¸ª
    path_is_valid: bool = False  # æ¨ç†è·¯å¾„æ˜¯å¦å®Œå…¨æ­£ç¡®
    dependency_nodes: List[str] = None  # ä¾èµ–çš„å‰ç½®èŠ‚ç‚¹åˆ—è¡¨
    invalid_dependencies: List[str] = None  # æ— æ•ˆçš„ä¾èµ–èŠ‚ç‚¹
    reasoning_quality: str = "unknown"  # æ¨ç†è´¨é‡ (perfect/partial/invalid)
    
    def __post_init__(self):
        if self.dependency_nodes is None:
            self.dependency_nodes = []
        if self.invalid_dependencies is None:
            self.invalid_dependencies = []
    
    def __str__(self):
        return f"StatementNode('{self.original_statement}', correct={self.is_correct}, path_valid={self.path_is_valid}, quality={self.reasoning_quality})"


class StatementProcessor:
    """Statementå¤„ç†å™¨"""
    
    def __init__(self):
        self.condition_list: List[str] = []      # æ¡ä»¶åˆ—è¡¨
        self.declared_list: List[str] = []       # å£°æ˜åˆ—è¡¨
        self.node_records: Dict[str, NodeRecord] = {}  # èŠ‚ç‚¹è®°å½•
    
    def parse_output_entities(self, output_part: str) -> Dict[str, Any]:
        """è§£æoutputéƒ¨åˆ†çš„å®ä½“ç»“æ„"""
        output_part = output_part.strip()
        
        if " and " in output_part:
            entities = [e.strip() for e in output_part.split(" and ")]
            return {"type": "and", "entities": entities}
        elif " or " in output_part:
            entities = [e.strip() for e in output_part.split(" or ")]
            return {"type": "or", "entities": entities}
        else:
            return {"type": "single", "entities": [output_part]}
    
    def extract_initial_conditions(self, original_question: str) -> List[str]:
        """
        ä»åŸå§‹é—®é¢˜ä¸­æå–åˆå§‹æ¡ä»¶
        
        Args:
            original_question: åŸå§‹é—®é¢˜æ–‡æœ¬
            
        Returns:
            åˆå§‹æ¡ä»¶åˆ—è¡¨
        """
        conditions = []
        
        # æå–Given Informationéƒ¨åˆ†
        given_match = re.search(r'\*\*Given Information\*\*:\s*(.*?)\s*\*\*', 
                               original_question, re.DOTALL)
        
        if given_match:
            given_text = given_match.group(1).strip()
            
            # æŒ‰å¥å·åˆ†å‰²æ¡ä»¶
            raw_conditions = re.split(r'\.\s*', given_text)
            
            for condition in raw_conditions:
                condition = condition.strip()
                if condition and ' is ' in condition:
                    conditions.append(condition)
        
        return conditions
    
    def clean_statement(self, statement: str) -> str:
        """
        æ¸…æ´—statementæ ¼å¼ï¼Œå°†ä¸æ ‡å‡†çš„è¡¨è¾¾è½¬æ¢ä¸ºæ ‡å‡†çš„"A is B"æ ¼å¼
        
        Args:
            statement: åŸå§‹statement
            
        Returns:
            æ¸…æ´—åçš„statement
        """
        if not statement or not isinstance(statement, str):
            return statement
        
        # å»é™¤é¦–å°¾ç©ºæ ¼
        cleaned = statement.strip()
        
        # æ›¿æ¢å„ç§ç­‰ä»·è¡¨è¾¾ä¸ºæ ‡å‡†çš„"is"
        replacements = [
            (" is connected to ", " is "),
            (" is in ", " is "),
            (" belongs to ", " is "),
            (" is contained in ", " is "),
            (" is part of ", " is "),
            (" is under ", " is "),
            (" is also under ", " is "),
            (" leads to ", " is "),
            (" connects to ", " is "),
            (" is from ", " is "),  # æ–°å¢
            (" from ", ""),  # ç§»é™¤fromå‰ç¼€
            (" is not in ", " is not "),  # å¤„ç†å¦å®š
            (" is not ", " is not "),  # å¤„ç†å¦å®š
        ]
        
        for old_phrase, new_phrase in replacements:
            cleaned = cleaned.replace(old_phrase, new_phrase)
        
        # å¤„ç† "from A is B" è¿™ç§æ ¼å¼ â†’ "A is B"
        if cleaned.startswith("from "):
            cleaned = cleaned[5:].strip()
        
        # è¿‡æ»¤åŒ…å«"not"çš„è¯­å¥ï¼Œå› ä¸ºæˆ‘ä»¬çš„æ¨ç†ç³»ç»Ÿä¸å¤„ç†å¦å®š
        if " not " in cleaned:
            return ""
        
        # å¤„ç†"A includes B" â†’ "B is A"çš„æƒ…å†µ
        if " includes " in cleaned:
            parts = cleaned.split(" includes ", 1)
            if len(parts) == 2:
                a, b = parts[0].strip(), parts[1].strip()
                cleaned = f"{b} is {a}"
        
        # å»é™¤ä»£è¯å’Œä¸åˆæ ¼çš„è¡¨è¾¾
        pronouns_to_remove = ["it ", "this ", "that "]
        for pronoun in pronouns_to_remove:
            if cleaned.startswith(pronoun):
                # å¦‚æœä»¥ä»£è¯å¼€å¤´ï¼Œæ ‡è®°ä¸ºæ— æ•ˆ
                return ""
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‡ªæŒ‡è¯­å¥
        if " is " in cleaned:
            parts = cleaned.split(" is ", 1)
            if len(parts) == 2:
                a, b = parts[0].strip(), parts[1].strip()
                if a == b:
                    # è‡ªæŒ‡è¯­å¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºåº”è¯¥è¢«è¿‡æ»¤
                    return ""
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆçš„å®ä½“æ ¼å¼
                if not self.is_valid_entity(a):
                    return ""
        
        # æœ€ç»ˆæ ¼å¼éªŒè¯ï¼šå¿…é¡»æ˜¯ "A is B" æ ¼å¼ä¸”Aå’ŒBéƒ½æ˜¯æœ‰æ•ˆå®ä½“
        if " is " not in cleaned:
            return ""
        
        return cleaned
    
    def is_valid_entity(self, entity: str) -> bool:
        """
        æ£€æŸ¥å®ä½“æ˜¯å¦ç¬¦åˆæ ¼å¼è¦æ±‚
        
        Args:
            entity: å®ä½“åç§°
            
        Returns:
            æ˜¯å¦ä¸ºæœ‰æ•ˆå®ä½“
        """
        if not entity or not isinstance(entity, str):
            return False
        
        entity = entity.strip()
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºxæˆ–ä»¥pusç»“å°¾
        if entity == "x" or entity.endswith("pus"):
            return True
        
        return False
    
    def expand_chain_statement(self, statement: str) -> List[str]:
        """
        å°†é“¾å¼è¯­å¥å±•å¼€ä¸ºå¤šä¸ª"A is B"æ ¼å¼çš„è¯­å¥
        
        Args:
            statement: é“¾å¼è¯­å¥ï¼Œå¦‚"xâ†’Aâ†’Bâ†’C"
            
        Returns:
            å±•å¼€åçš„è¯­å¥åˆ—è¡¨ï¼Œå¦‚["x is A", "A is B", "B is C", "x is C"]
        """
        if "â†’" not in statement:
            return [statement]
        
        # åˆ†å‰²é“¾å¼èŠ‚ç‚¹
        nodes = [node.strip() for node in statement.split("â†’")]
        
        if len(nodes) < 2:
            return [statement]
        
        # ç”Ÿæˆ"A is B"æ ¼å¼çš„è¯­å¥
        expanded_statements = []
        
        # 1. ç”Ÿæˆç›¸é‚»èŠ‚ç‚¹çš„è¿æ¥å…³ç³»
        for i in range(len(nodes) - 1):
            input_node = nodes[i]
            output_node = nodes[i + 1]
            
            # æ£€æŸ¥å®ä½“æœ‰æ•ˆæ€§
            if self.is_valid_entity(input_node) and self.is_valid_entity(output_node):
                expanded_statements.append(f"{input_node} is {output_node}")
        
        # 2. æ·»åŠ é¦–å°¾è¿æ¥å…³ç³»ï¼ˆå¦‚æœé“¾é•¿åº¦å¤§äº2ï¼‰
        if len(nodes) > 2:
            first_node = nodes[0]
            last_node = nodes[-1]
            
            # æ£€æŸ¥é¦–å°¾èŠ‚ç‚¹çš„æœ‰æ•ˆæ€§
            if self.is_valid_entity(first_node) and self.is_valid_entity(last_node):
                # é¿å…é‡å¤æ·»åŠ ï¼ˆå¦‚æœé“¾é•¿åº¦ä¸º2ï¼Œé¦–å°¾è¿æ¥å·²ç»åœ¨ä¸Šé¢æ·»åŠ è¿‡äº†ï¼‰
                first_to_last = f"{first_node} is {last_node}"
                if first_to_last not in expanded_statements:
                    expanded_statements.append(first_to_last)
        
        return expanded_statements
    
    def parse_statement_to_node(self, statement: str, stmt_type: str) -> Dict[str, Any]:
        """
        å°†statementè§£æä¸ºèŠ‚ç‚¹æ ¼å¼
        
        Args:
            statement: statementå­—ç¬¦ä¸²
            stmt_type: statementç±»å‹
            
        Returns:
            èŠ‚ç‚¹å­—å…¸ï¼ŒåŒ…å«original, input, output, output_parsedå­—æ®µ
        """
        if stmt_type == "planning" and " is " not in statement:
            # å®šä¹‰è§„åˆ’æ ¼å¼ï¼š"A" â†’ æŸ¥æ‰¾Açš„å®šä¹‰
            if self.is_valid_entity(statement):
                return {
                    "original": statement,
                    "input": statement,
                    "output": "?",  # è¡¨ç¤ºå¾…æŸ¥æ‰¾
                    "output_parsed": {"type": "unknown", "entities": []},
                    "type": stmt_type
                }
            else:
                return None  # æ— æ•ˆå®ä½“ï¼Œè¿‡æ»¤æ‰
        
        if " is " not in statement:
            return None  # ä¸ç¬¦åˆ"A is B"æ ¼å¼
        
        # åˆ†å‰²"A is B"
        parts = statement.split(" is ", 1)
        if len(parts) != 2:
            return None
        
        input_part = parts[0].strip()
        output_part = parts[1].strip()
        
        # æ£€æŸ¥inputéƒ¨åˆ†çš„æœ‰æ•ˆæ€§
        if not self.is_valid_entity(input_part):
            return None  # inputæ— æ•ˆï¼Œè¿‡æ»¤æ‰
        
        # è§£æoutputéƒ¨åˆ†çš„ç»“æ„
        output_parsed = self.parse_output_entities(output_part)
        
        # æ£€æŸ¥outputéƒ¨åˆ†çš„æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨è§£æåçš„å®ä½“åˆ—è¡¨ï¼‰
        valid_entities = [e for e in output_parsed["entities"] if self.is_valid_entity(e)]
        if len(valid_entities) != len(output_parsed["entities"]):
            return None  # æœ‰æ— æ•ˆå®ä½“ï¼Œè¿‡æ»¤æ‰
        
        return {
            "original": statement,
            "input": input_part,
            "output": output_part,
            "output_parsed": output_parsed,  # æ–°å¢ï¼šè§£æåçš„outputç»“æ„
            "type": stmt_type
        }
    
    def normalize_and_parse_statements(self, statements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ ‡å‡†åŒ–å¹¶è§£æstatementsä¸ºèŠ‚ç‚¹æ ¼å¼
        
        Args:
            statements: æ¸…æ´—åçš„statementsåˆ—è¡¨
            
        Returns:
            æ ‡å‡†åŒ–çš„èŠ‚ç‚¹åˆ—è¡¨
        """
        normalized_nodes = []
        
        for stmt_dict in statements:
            stmt_type = stmt_dict.get('type', 'unknown')
            statement = stmt_dict.get('statement', '')
            
            if not statement:
                continue
            
            # 1. å¤„ç†é“¾å¼è¯­å¥
            if "â†’" in statement:
                expanded_statements = self.expand_chain_statement(statement)
                for expanded_stmt in expanded_statements:
                    node = self.parse_statement_to_node(expanded_stmt, stmt_type)
                    if node:
                        normalized_nodes.append(node)
            else:
                # 2. å¤„ç†æ™®é€šè¯­å¥
                node = self.parse_statement_to_node(statement, stmt_type)
                if node:
                    normalized_nodes.append(node)
        
        return normalized_nodes
    
    def clean_statements_list(self, statements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        æ¸…æ´—statementsåˆ—è¡¨ï¼Œæ ‡å‡†åŒ–æ ¼å¼å¹¶è¿‡æ»¤æ— æ•ˆstatements
        
        Args:
            statements: åŸå§‹statementsåˆ—è¡¨
            
        Returns:
            æ¸…æ´—åçš„statementsåˆ—è¡¨
        """
        cleaned_statements = []
        
        for stmt_dict in statements:
            if not isinstance(stmt_dict, dict):
                # å…¼å®¹æ—§æ ¼å¼
                continue
            
            stmt_type = stmt_dict.get('type', 'unknown')
            statement = stmt_dict.get('statement', '')
            
            if not statement:
                continue
            
            # æ¸…æ´—statement
            cleaned_statement = self.clean_statement(statement)
            
            # å¦‚æœæ¸…æ´—åä¸ºç©ºï¼Œè·³è¿‡
            if not cleaned_statement:
                continue
            
            # å¯¹äºplanningç±»å‹ï¼Œè¿›è¡Œé¢å¤–çš„æ ¼å¼æ£€æŸ¥
            if stmt_type == 'planning':
                # æ£€æŸ¥æ˜¯å¦ç¬¦åˆä¸¤ç§åˆæ³•æ ¼å¼ï¼š
                # 1. "A is B" (ç›®æ ‡/è¿æ¥è§„åˆ’)
                # 2. "A" (å®šä¹‰è§„åˆ’)
                
                if " is " in cleaned_statement:
                    # æ ¼å¼1ï¼šç›®æ ‡/è¿æ¥è§„åˆ’
                    parts = cleaned_statement.split(" is ", 1)
                    if len(parts) == 2:
                        a, b = parts[0].strip(), parts[1].strip()
                        if a and b:
                            cleaned_statements.append({
                                "type": stmt_type,
                                "statement": cleaned_statement
                            })
                elif cleaned_statement and " " not in cleaned_statement:
                    # æ ¼å¼2ï¼šå®šä¹‰è§„åˆ’ï¼ˆå•ä¸ªæ¦‚å¿µï¼‰
                    cleaned_statements.append({
                        "type": stmt_type,
                        "statement": cleaned_statement
                    })
                # å…¶ä»–æ ¼å¼çš„planningè¯­å¥è¢«è¿‡æ»¤æ‰
            else:
                # actualç±»å‹ä¿æŒæ¸…æ´—åçš„ç»“æœ
                cleaned_statements.append({
                    "type": stmt_type,
                    "statement": cleaned_statement
                })
        
        return cleaned_statements
    
    def can_derive_from_conditions(self, declared_conditions: List[str], current_statement: str) -> bool:
        """
        åˆ¤æ–­å½“å‰é™ˆè¿°æ˜¯å¦å¯ä»¥ä»å·²å£°æ˜çš„æ¡ä»¶ä¸­æ¨å¯¼å‡ºæ¥
        
        Args:
            declared_conditions: å·²å£°æ˜çš„æ¡ä»¶åˆ—è¡¨
            current_statement: å½“å‰é™ˆè¿°
            
        Returns:
            æ˜¯å¦å¯ä»¥æ¨å¯¼å‡ºæ¥
        """
        # TODO: è¿™ä¸ªå‡½æ•°éœ€è¦ç”¨æˆ·å®ç°å…·ä½“çš„æ¨å¯¼é€»è¾‘
        # ç›®å‰è¿”å›Falseä½œä¸ºå ä½ç¬¦
        return False
    
    def process_statements(self, statements: List[str], debug_mode: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†statementsåˆ—è¡¨ï¼Œæ‰§è¡Œåå¤„ç†é€»è¾‘
        
        Args:
            statements: statementåˆ—è¡¨
            debug_mode: è°ƒè¯•æ¨¡å¼
            
        Returns:
            å¤„ç†ç»“æœ
        """
        if debug_mode:
            # è°ƒè¯•æ¨¡å¼ï¼šåªè®°å½•ï¼Œä¸æ‰§è¡Œå®é™…é€»è¾‘
            return {
                "debug_mode": True,
                "total_statements": len(statements),
                "statements": statements,
                "message": "è°ƒè¯•æ¨¡å¼ï¼šè·³è¿‡åå¤„ç†é€»è¾‘"
            }
        
        # TODO: å®ç°å®Œæ•´çš„åå¤„ç†é€»è¾‘
        results = {
            "condition_list": self.condition_list,
            "declared_list": self.declared_list,
            "node_records": {},
            "processing_complete": False
        }
        
        # è½¬æ¢node_recordsä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        for statement, record in self.node_records.items():
            results["node_records"][statement] = {
                "statement": record.statement,
                "positions": record.positions,
                "status_history": [status.value for status in record.status_history],
                "occurrence_count": record.occurrence_count
            }
        
        return results


class LogicalReasoningEngine:
    """é€»è¾‘æ¨ç†å¼•æ“ - é›†æˆè‡ªdebug_test.pyçš„æ¨ç†ç®—æ³•"""
    
    def __init__(self, max_depth: int = 2000, timeout: int = 120):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            max_depth: æœ€å¤§æ¨ç†æ·±åº¦
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_depth = max_depth
        self.timeout = timeout
    
    def parse_output_entities(self, output_part: str) -> Dict[str, Any]:
        """è§£æoutputéƒ¨åˆ†çš„å®ä½“ç»“æ„"""
        output_part = output_part.strip()
        
        if " and " in output_part:
            entities = [e.strip() for e in output_part.split(" and ")]
            return {"type": "and", "entities": entities}
        elif " or " in output_part:
            entities = [e.strip() for e in output_part.split(" or ")]
            return {"type": "or", "entities": entities}
        else:
            return {"type": "single", "entities": [output_part]}
    
    def statements_equal(self, stmt1: Dict[str, Any], stmt2: Dict[str, Any]) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªè¯­å¥æ˜¯å¦ç›¸ç­‰"""
        return (stmt1["input"] == stmt2["input"] and 
                stmt1["output"] == stmt2["output"])
    
    def is_provable(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                   visited: set = None, depth: int = 0, start_time: float = None, 
                   debug: bool = False) -> bool:
        """
        åå‘æ¨ç†ï¼šåˆ¤æ–­ç›®æ ‡æ˜¯å¦å¯ä»¥ä»å‰æä¸­æ¨å¯¼å‡ºæ¥
        
        Args:
            target: ç›®æ ‡ç»“è®ºï¼Œæ ¼å¼ä¸ºèŠ‚ç‚¹å­—å…¸
            premises: å‰ææ¡ä»¶åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºèŠ‚ç‚¹å­—å…¸
            visited: å·²è®¿é—®çš„ç›®æ ‡é›†åˆï¼Œé˜²æ­¢å¾ªç¯
            depth: é€’å½’æ·±åº¦
            start_time: å¼€å§‹æ—¶é—´
            debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
            
        Returns:
            æ˜¯å¦å¯ä»¥æ¨å¯¼å‡ºæ¥
        """
        if visited is None:
            visited = set()
        
        if start_time is None:
            start_time = time.time()
        
        # è¶…æ—¶æ£€æŸ¥
        if time.time() - start_time > self.timeout:
            if debug:
                print(f"æ¨ç†è¶…æ—¶({self.timeout}s)ï¼Œç»ˆæ­¢")
            return False
        
        indent = "  " * depth
        if debug:
            print(f"{indent}å°è¯•è¯æ˜: {target['input']} is {target['output']}")
        
        # æ”¹è¿›çš„å¾ªç¯æ£€æµ‹ï¼šé˜²æ­¢æ— é™é€’å½’
        target_key = f"{target['input']}â†’{target['output']}"
        
        # å¦‚æœåœ¨å½“å‰è·¯å¾„ä¸­å·²ç»è®¿é—®è¿‡è¿™ä¸ªç›®æ ‡ï¼Œç›´æ¥è·³è¿‡
        if target_key in visited:
            if debug:
                print(f"{indent}æ£€æµ‹åˆ°å¾ªç¯ä¾èµ–ï¼Œè·³è¿‡")
            return False
        
        if depth > self.max_depth:
            if debug:
                print(f"{indent}è¶…è¿‡æœ€å¤§æ·±åº¦({self.max_depth})ï¼Œè·³è¿‡")
            return False
        
        # ä¸´æ—¶æ·»åŠ åˆ°visited
        visited.add(target_key)
        
        try:
            # åŸºç¡€æƒ…å†µï¼šç›®æ ‡å·²ç»åœ¨å‰æä¸­
            for premise in premises:
                if self.statements_equal(target, premise):
                    if debug:
                        print(f"{indent}âœ“ åœ¨å‰æä¸­æ‰¾åˆ°: {premise.get('original', premise['input'] + ' is ' + premise['output'])}")
                    return True
            
            # å¯»æ‰¾å¯èƒ½çš„æ¨ç†è·¯å¾„
            possible_paths = self.find_reasoning_paths(target, premises, debug and depth < 5)
            
            if debug:
                print(f"{indent}æ‰¾åˆ° {len(possible_paths)} ç§æ¨ç†è·¯å¾„")
            
            # æŒ‰ä¼˜å…ˆçº§æ’åºè·¯å¾„ï¼šä¼˜å…ˆå°è¯•ç®€å•çš„è·¯å¾„
            def path_priority(path):
                rule_priority = {
                    'CE': 1, 'DI_EXPAND': 2, 'DI': 3, 'MP': 4, 'CI': 5, 'MP+CE': 6
                }
                return (len(path['intermediates']), rule_priority.get(path['rule'], 10))
            
            possible_paths.sort(key=path_priority)
            
            # å°è¯•æ‰€æœ‰æ¨ç†è·¯å¾„ï¼Œåªå—è¶…æ—¶é™åˆ¶
            for i, path in enumerate(possible_paths):
                # è¶…æ—¶æ£€æŸ¥
                if time.time() - start_time > self.timeout:
                    if debug:
                        print(f"{indent}æ¨ç†è¶…æ—¶ï¼Œåœæ­¢å°è¯•æ›´å¤šè·¯å¾„")
                    break
                    
                if debug:
                    print(f"{indent}å°è¯•è·¯å¾„ {i+1}: {path['rule']}")
                    for j, intermediate in enumerate(path['intermediates']):
                        print(f"{indent}  éœ€è¦: {intermediate['input']} is {intermediate['output']}")
                
                # æ£€æŸ¥è¿™æ¡è·¯å¾„çš„æ‰€æœ‰ä¸­é—´æ­¥éª¤æ˜¯å¦éƒ½å¯ä»¥è¯æ˜
                all_provable = True
                for intermediate in path['intermediates']:
                    # ä½¿ç”¨å½“å‰visitedçš„å‰¯æœ¬ï¼Œé¿å…å½±å“å…¶ä»–è·¯å¾„
                    if not self.is_provable(intermediate, premises, visited.copy(), 
                                          depth + 1, start_time, debug):
                        all_provable = False
                        break
                
                if all_provable:
                    if debug:
                        print(f"{indent}âœ“ è·¯å¾„ {i+1} æˆåŠŸ")
                    return True
                elif debug:
                    print(f"{indent}âœ— è·¯å¾„ {i+1} å¤±è´¥")
            
            if debug:
                print(f"{indent}âœ— æ‰€æœ‰è·¯å¾„éƒ½å¤±è´¥")
            return False
            
        finally:
            # ç§»é™¤å½“å‰ç›®æ ‡çš„è®¿é—®è®°å½•ï¼Œå…è®¸å…¶ä»–è·¯å¾„è®¿é—®
            visited.discard(target_key)
    
    def find_reasoning_paths(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                           debug: bool = False) -> List[Dict[str, Any]]:
        """ä»å·²æœ‰å‰æä¸­å¯»æ‰¾å¯ä»¥æ¨å¯¼å‡ºç›®æ ‡çš„æ¨ç†è·¯å¾„"""
        paths = []
        target_input = target["input"]
        target_output = target["output"]
        target_output_parsed = target["output_parsed"]
        
        # è§„åˆ™1: MP (Modus Ponens)
        mp_paths = self.find_mp_paths(target, premises, debug)
        paths.extend(mp_paths)
        
        # è§„åˆ™2: CE (Conjunction Elimination) 
        ce_paths = self.find_ce_paths(target, premises, debug)
        paths.extend(ce_paths)
        
        # è§„åˆ™3: CI (Conjunction Introduction)
        ci_paths = self.find_ci_paths(target, premises, debug)
        paths.extend(ci_paths)
        
        # è§„åˆ™4: DI (Disjunction Introduction)
        di_paths = self.find_di_paths(target, premises, debug)
        paths.extend(di_paths)
        
        return paths
    
    def find_mp_paths(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                     debug: bool = False) -> List[Dict[str, Any]]:
        """å¯»æ‰¾MPè§„åˆ™çš„æ¨ç†è·¯å¾„"""
        paths = []
        target_input = target["input"]
        target_output = target["output"]
        target_output_parsed = target["output_parsed"]
        
        # å¯»æ‰¾å½¢å¦‚ target_input is X çš„å‰æ
        for premise in premises:
            if premise["input"] == target_input:
                # æ‰¾åˆ°äº† target_input is X
                x_value = premise["output"]
                x_parsed = premise["output_parsed"]
                
                # æƒ…å†µ1: å¦‚æœXæ˜¯å•ä¸ªå€¼ï¼Œå¯»æ‰¾ X is target_output
                if x_parsed["type"] == "single":
                    intermediate_target = {
                        "input": x_value,
                        "output": target_output,
                        "output_parsed": target_output_parsed,
                        "original": f"{x_value} is {target_output}",
                        "type": "intermediate"
                    }
                    paths.append({
                        "rule": "MP",
                        "intermediates": [intermediate_target]
                    })
                    
                    if debug:
                        print(f"    MPè·¯å¾„: {target_input} is {x_value} â†’ éœ€è¦è¯æ˜ {x_value} is {target_output}")
                
                # æƒ…å†µ2: å¦‚æœXæ˜¯å¤åˆå€¼(å¦‚A and B)ï¼Œå¯ä»¥é€šè¿‡CEæå–å•ä¸ªéƒ¨åˆ†ï¼Œç„¶åç»§ç»­MP
                elif x_parsed["type"] == "and":
                    for entity in x_parsed["entities"]:
                        # å…ˆé€šè¿‡CEå¾—åˆ° target_input is entityï¼Œå†é€šè¿‡MPå¾—åˆ°æœ€ç»ˆç›®æ ‡
                        ce_intermediate = {
                            "input": target_input,
                            "output": entity,
                            "output_parsed": {"type": "single", "entities": [entity]},
                            "original": f"{target_input} is {entity}",
                            "type": "intermediate"
                        }
                        mp_intermediate = {
                            "input": entity,
                            "output": target_output,
                            "output_parsed": target_output_parsed,
                            "original": f"{entity} is {target_output}",
                            "type": "intermediate"
                        }
                        
                        paths.append({
                            "rule": "MP+CE",
                            "intermediates": [ce_intermediate, mp_intermediate]
                        })
                        
                        if debug:
                            print(f"    MP+CEè·¯å¾„: {target_input} is {x_value} â†’ CEå¾—åˆ° {target_input} is {entity} â†’ MPéœ€è¦ {entity} is {target_output}")
        
        return paths
    
    def find_ce_paths(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                     debug: bool = False) -> List[Dict[str, Any]]:
        """å¯»æ‰¾CEè§„åˆ™çš„æ¨ç†è·¯å¾„"""
        paths = []
        target_input = target["input"]
        target_output = target["output"]
        target_output_parsed = target["output_parsed"]
        
        # å¦‚æœç›®æ ‡æ˜¯å•ä¸ªå®ä½“ï¼Œå¯»æ‰¾åŒ…å«å®ƒçš„å¤åˆæ¡ä»¶
        if target_output_parsed["type"] == "single":
            target_entity = target_output_parsed["entities"][0]
            
            # å¯»æ‰¾å½¢å¦‚ target_input is ... and target_entity ... çš„æ¡ä»¶
            for premise in premises:
                if (premise["input"] == target_input and 
                    premise["output_parsed"]["type"] == "and" and
                    target_entity in premise["output_parsed"]["entities"]):
                    
                    # æ‰¾åˆ°äº†å¯ä»¥é€šè¿‡CEå¾—åˆ°ç›®æ ‡çš„å¤åˆæ¡ä»¶
                    intermediate = {
                        "input": target_input,
                        "output": premise["output"],
                        "output_parsed": premise["output_parsed"],
                        "original": premise.get("original", f"{target_input} is {premise['output']}"),
                        "type": "intermediate"
                    }
                    paths.append({
                        "rule": "CE",
                        "intermediates": [intermediate]
                    })
                    
                    if debug:
                        print(f"    CEè·¯å¾„: {premise['output']} contains {target_output}")
        
        return paths
    
    def find_ci_paths(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                     debug: bool = False) -> List[Dict[str, Any]]:
        """å¯»æ‰¾CIè§„åˆ™çš„æ¨ç†è·¯å¾„"""
        paths = []
        target_input = target["input"]
        target_output_parsed = target["output_parsed"]
        
        # åªå¯¹andç±»å‹çš„ç›®æ ‡åº”ç”¨CIè§„åˆ™
        if target_output_parsed["type"] == "and":
            entities = target_output_parsed["entities"]
            
            # éœ€è¦ä¸ºæ¯ä¸ªå®ä½“æ‰¾åˆ°å•ç‹¬çš„æ¡ä»¶
            intermediates = []
            for entity in entities:
                intermediate = {
                    "input": target_input,
                    "output": entity,
                    "output_parsed": {"type": "single", "entities": [entity]},
                    "original": f"{target_input} is {entity}",
                    "type": "intermediate"
                }
                intermediates.append(intermediate)
            
            paths.append({
                "rule": "CI",
                "intermediates": intermediates
            })
            
            if debug:
                print(f"    CIè·¯å¾„: éœ€è¦ {len(entities)} ä¸ªå•ç‹¬çš„æ¡ä»¶")
        
        return paths
    
    def find_di_paths(self, target: Dict[str, Any], premises: List[Dict[str, Any]], 
                     debug: bool = False) -> List[Dict[str, Any]]:
        """å¯»æ‰¾DIè§„åˆ™çš„æ¨ç†è·¯å¾„"""
        paths = []
        target_input = target["input"]
        target_output_parsed = target["output_parsed"]
        
        # åªå¯¹orç±»å‹çš„ç›®æ ‡åº”ç”¨DIè§„åˆ™
        if target_output_parsed["type"] == "or":
            target_entities = set(target_output_parsed["entities"])
            
            # æ–¹æ³•1: ä¸ºæ¯ä¸ªå®ä½“åˆ›å»ºä¸€ä¸ªå•ç‹¬çš„è·¯å¾„ï¼ˆåªéœ€è¦å…¶ä¸­ä¸€ä¸ªæˆåŠŸï¼‰
            for entity in target_entities:
                intermediate = {
                    "input": target_input,
                    "output": entity,
                    "output_parsed": {"type": "single", "entities": [entity]},
                    "original": f"{target_input} is {entity}",
                    "type": "intermediate"
                }
                
                paths.append({
                    "rule": "DI",
                    "intermediates": [intermediate]
                })
                
                if debug:
                    print(f"    DIè·¯å¾„: åªéœ€è¦è¯æ˜ {target_input} is {entity}")
            
            # æ–¹æ³•2: å¯»æ‰¾åŒ…å«ç›®æ ‡å®ä½“å­é›†çš„oræ¡ä»¶ï¼ˆDIæ‰©å±•ï¼‰
            for premise in premises:
                if (premise["input"] == target_input and 
                    premise["output_parsed"]["type"] == "or"):
                    premise_entities = set(premise["output_parsed"]["entities"])
                    
                    # å¦‚æœå‰æçš„å®ä½“æ˜¯ç›®æ ‡å®ä½“çš„å­é›†ï¼Œå¯ä»¥é€šè¿‡DIæ‰©å±•
                    if premise_entities.issubset(target_entities) and premise_entities != target_entities:
                        intermediate = {
                            "input": target_input,
                            "output": premise["output"],
                            "output_parsed": premise["output_parsed"],
                            "original": premise.get("original", f"{target_input} is {premise['output']}"),
                            "type": "intermediate"
                        }
                        
                        paths.append({
                            "rule": "DI_EXPAND",
                            "intermediates": [intermediate]
                        })
                        
                        if debug:
                            print(f"    DIæ‰©å±•è·¯å¾„: ä» {target_input} is {premise['output']} æ‰©å±•åˆ° {target['output']}")
        
        return paths


class PostProcessor:
    """åå¤„ç†å™¨ - å¤„ç†Statementåˆ—è¡¨å’ŒLoGå›¾éªŒè¯"""
    
    def __init__(self, reasoning_engine: LogicalReasoningEngine):
        """
        åˆå§‹åŒ–åå¤„ç†å™¨
        
        Args:
            reasoning_engine: æ¨ç†å¼•æ“å®ä¾‹
        """
        self.reasoning_engine = reasoning_engine
        self.statement_list: List[StatementNode] = []  # StatementèŠ‚ç‚¹åˆ—è¡¨
        self.log_graph: List[Dict[str, Any]] = []      # LoGæ ‡å‡†ç­”æ¡ˆå›¾
        self.illuminated_nodes: set = set()            # å·²ç‚¹äº®çš„LoGèŠ‚ç‚¹
        
    def load_log_graph(self, graph_data: List[Dict[str, Any]]):
        """
        åŠ è½½LoGæ ‡å‡†ç­”æ¡ˆå›¾
        
        Args:
            graph_data: å›¾æ•°æ®åˆ—è¡¨
        """
        self.log_graph = graph_data
        self.illuminated_nodes = set()
        print(f"[åå¤„ç†] åŠ è½½LoGå›¾ï¼ŒåŒ…å« {len(self.log_graph)} ä¸ªèŠ‚ç‚¹")
        
        # æ‰“å°å›¾ç»“æ„ä»¥ä¾¿è°ƒè¯•
        for i, node in enumerate(self.log_graph):
            print(f"  LoGèŠ‚ç‚¹ {i}: {node.get('output', 'N/A')} (è§„åˆ™: {node.get('deduction_rule', 'N/A')}, æ·±åº¦: {node.get('depth', 'N/A')})")
    
    def find_statement_node(self, target_statement: str) -> Optional[StatementNode]:
        """
        åœ¨statementåˆ—è¡¨ä¸­æŸ¥æ‰¾èŠ‚ç‚¹
        
        Args:
            target_statement: ç›®æ ‡è¯­å¥
            
        Returns:
            æ‰¾åˆ°çš„èŠ‚ç‚¹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        for node in self.statement_list:
            if node.original_statement == target_statement:
                return node
        return None
    
    def is_in_premises(self, statement: str, initial_conditions: List[str]) -> bool:
        """
        æ£€æŸ¥è¯­å¥æ˜¯å¦åœ¨åˆå§‹å‰ææ¡ä»¶ä¸­
        
        Args:
            statement: è¯­å¥
            initial_conditions: åˆå§‹æ¡ä»¶åˆ—è¡¨
            
        Returns:
            æ˜¯å¦åœ¨å‰æä¸­
        """
        return statement in initial_conditions
    
    def create_statement_node(self, original_statement: str, input_entity: str, 
                            output_entity: str, output_parsed: Dict[str, Any], 
                            occurrence_index: int) -> StatementNode:
        """
        åˆ›å»ºæ–°çš„StatementèŠ‚ç‚¹
        
        Args:
            original_statement: åŸå§‹è¯­å¥
            input_entity: è¾“å…¥å®ä½“
            output_entity: è¾“å‡ºå®ä½“
            output_parsed: è§£æåçš„è¾“å‡ºç»“æ„
            occurrence_index: å‡ºç°ç´¢å¼•
            
        Returns:
            æ–°åˆ›å»ºçš„StatementèŠ‚ç‚¹
        """
        return StatementNode(
            original_statement=original_statement,
            input_entity=input_entity,
            output_entity=output_entity,
            output_parsed=output_parsed,
            occurrence_count=1,
            is_correct=False,
            is_premise=False,
            node_type="unknown",
            first_occurrence_index=occurrence_index
        )
    
    def find_corresponding_log_node(self, statement: str) -> Optional[Dict[str, Any]]:
        """
        åœ¨LoGå›¾ä¸­æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹
        
        Args:
            statement: å®Œæ•´çš„è¯­å¥ (å¦‚ "x is B")
            
        Returns:
            å¯¹åº”çš„LoGèŠ‚ç‚¹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        for node in self.log_graph:
            if node.get('output', '') == statement:
                return node
        return None
    
    def illuminate_log_node(self, log_node: Dict[str, Any]):
        """
        ç‚¹äº®LoGèŠ‚ç‚¹ï¼Œå¹¶è‡ªåŠ¨ç‚¹äº®å…¶æ‰€æœ‰å­èŠ‚ç‚¹
        
        Args:
            log_node: LoGèŠ‚ç‚¹
        """
        node_id = log_node.get('output', '')
        if node_id not in self.illuminated_nodes:
            self.illuminated_nodes.add(node_id)
            print(f"[åå¤„ç†] ç‚¹äº®LoGèŠ‚ç‚¹: {node_id}")
            
            # è‡ªåŠ¨ç‚¹äº®æ‰€æœ‰å­èŠ‚ç‚¹ï¼ˆæ·±åº¦æ›´å¤§çš„ä¾èµ–èŠ‚ç‚¹ï¼‰
            self.auto_illuminate_children(log_node)
    
    def auto_illuminate_children(self, parent_node: Dict[str, Any]):
        """
        è‡ªåŠ¨ç‚¹äº®çˆ¶èŠ‚ç‚¹çš„æ‰€æœ‰å­èŠ‚ç‚¹
        
        Args:
            parent_node: çˆ¶èŠ‚ç‚¹
        """
        parent_depth = parent_node.get('depth', 0)
        parent_inputs = parent_node.get('input', [])
        
        if not isinstance(parent_inputs, list):
            return
        
        # æ‰¾åˆ°æ‰€æœ‰å­èŠ‚ç‚¹ï¼ˆè¾“å‡ºæ˜¯å½“å‰èŠ‚ç‚¹è¾“å…¥çš„èŠ‚ç‚¹ï¼‰
        children_illuminated = 0
        for input_statement in parent_inputs:
            for log_node in self.log_graph:
                if (log_node.get('output', '') == input_statement and 
                    log_node.get('depth', 0) > parent_depth):
                    
                    child_id = log_node.get('output', '')
                    if child_id not in self.illuminated_nodes:
                        self.illuminated_nodes.add(child_id)
                        children_illuminated += 1
                        print(f"[åå¤„ç†]   â””â”€ è‡ªåŠ¨ç‚¹äº®å­èŠ‚ç‚¹: {child_id}")
                        
                        # é€’å½’ç‚¹äº®æ›´æ·±å±‚çš„å­èŠ‚ç‚¹
                        self.auto_illuminate_children(log_node)
        
        if children_illuminated > 0:
            print(f"[åå¤„ç†] å…±è‡ªåŠ¨ç‚¹äº® {children_illuminated} ä¸ªå­èŠ‚ç‚¹")
    
    def get_correct_statements_as_premises(self) -> List[Dict[str, Any]]:
        """
        è·å–æ­£ç¡®çš„Statementä½œä¸ºå‰ææ¡ä»¶
        
        Returns:
            æ­£ç¡®Statementçš„å‰ææ ¼å¼åˆ—è¡¨
        """
        premises = []
        for stmt_node in self.statement_list:
            if stmt_node.is_correct:
                premise = {
                    "original": stmt_node.original_statement,
                    "input": stmt_node.input_entity,
                    "output": stmt_node.output_entity,
                    "output_parsed": stmt_node.output_parsed,
                    "type": "correct_statement"
                }
                premises.append(premise)
        return premises
    
    def get_all_statements_as_premises(self) -> List[Dict[str, Any]]:
        """
        è·å–æ‰€æœ‰Statementä½œä¸ºå‰ææ¡ä»¶ï¼ˆåŒ…æ‹¬é”™è¯¯çš„ï¼‰
        
        Returns:
            æ‰€æœ‰Statementçš„å‰ææ ¼å¼åˆ—è¡¨
        """
        premises = []
        for stmt_node in self.statement_list:
            premise = {
                "original": stmt_node.original_statement,
                "input": stmt_node.input_entity,
                "output": stmt_node.output_entity,
                "output_parsed": stmt_node.output_parsed,
                "type": "all_statement"
            }
            premises.append(premise)
        return premises
    
    def analyze_reasoning_path(self, target_node: StatementNode) -> Dict[str, Any]:
        """
        åˆ†æèŠ‚ç‚¹çš„æ¨ç†è·¯å¾„å®Œæ•´æ€§
        
        Args:
            target_node: ç›®æ ‡èŠ‚ç‚¹
            
        Returns:
            è·¯å¾„åˆ†æç»“æœ
        """
        if target_node.is_premise:
            # å‰æèŠ‚ç‚¹è·¯å¾„æ€»æ˜¯æœ‰æ•ˆçš„
            target_node.path_is_valid = True
            target_node.reasoning_quality = "perfect"
            return {"status": "premise", "dependencies": []}
        
        # æ‰¾åˆ°æ¨ç†è·¯å¾„ä¸­çš„æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹
        dependencies = []
        invalid_deps = []
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šæ£€æŸ¥ç›´æ¥ä¾èµ–
        # å¯¹äº "x is B"ï¼Œæ£€æŸ¥æ˜¯å¦å­˜åœ¨ "x is A" å’Œ "A is B" çš„è·¯å¾„
        input_entity = target_node.input_entity
        output_entity = target_node.output_entity
        
        # æŸ¥æ‰¾æ‰€æœ‰ä»¥input_entityå¼€å¤´çš„è¯­å¥
        input_statements = [s for s in self.statement_list if s.input_entity == input_entity and s != target_node]
        
        for stmt in input_statements:
            dependencies.append(stmt.original_statement)
            if not stmt.is_correct:
                invalid_deps.append(stmt.original_statement)
        
        # æŸ¥æ‰¾è¿æ¥åˆ°output_entityçš„è¯­å¥
        connecting_statements = [s for s in self.statement_list 
                               if s.output_entity == output_entity and s.input_entity != input_entity]
        
        for stmt in connecting_statements:
            dependencies.append(stmt.original_statement)
            if not stmt.is_correct:
                invalid_deps.append(stmt.original_statement)
        
        # æ›´æ–°èŠ‚ç‚¹ä¿¡æ¯
        target_node.dependency_nodes = dependencies
        target_node.invalid_dependencies = invalid_deps
        
        # åˆ¤æ–­æ¨ç†è´¨é‡
        if len(invalid_deps) == 0:
            target_node.path_is_valid = True
            target_node.reasoning_quality = "perfect"
        elif len(invalid_deps) < len(dependencies):
            target_node.path_is_valid = False
            target_node.reasoning_quality = "partial"
        else:
            target_node.path_is_valid = False
            target_node.reasoning_quality = "invalid"
        
        return {
            "status": "analyzed",
            "dependencies": dependencies,
            "invalid_dependencies": invalid_deps,
            "path_valid": target_node.path_is_valid,
            "quality": target_node.reasoning_quality
        }
    
    def process_nodes(self, normalized_nodes: List[Dict[str, Any]], 
                     initial_conditions: List[str]) -> Dict[str, Any]:
        """
        å¤„ç†æ ‡å‡†åŒ–èŠ‚ç‚¹åˆ—è¡¨
        
        Args:
            normalized_nodes: æ ‡å‡†åŒ–èŠ‚ç‚¹åˆ—è¡¨
            initial_conditions: åˆå§‹æ¡ä»¶åˆ—è¡¨
            
        Returns:
            å¤„ç†ç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"[åå¤„ç†] å¼€å§‹å¤„ç†èŠ‚ç‚¹")
        print(f"{'='*60}")
        
        print(f"ğŸ“Š åˆå§‹çŠ¶æ€:")
        print(f"   - æ ‡å‡†åŒ–èŠ‚ç‚¹æ•°: {len(normalized_nodes)}")
        print(f"   - åˆå§‹æ¡ä»¶æ•°: {len(initial_conditions)}")
        print(f"   - LoGå›¾èŠ‚ç‚¹æ•°: {len(self.log_graph)}")
        
        print(f"\nğŸ“‹ åˆå§‹æ¡ä»¶åˆ—è¡¨:")
        for i, condition in enumerate(initial_conditions):
            print(f"   {i+1:2d}. {condition}")
        
        # é¦–å…ˆå°†åˆå§‹æ¡ä»¶åŠ å…¥statementåˆ—è¡¨
        print(f"\nğŸ”§ æ„å»ºåˆå§‹Statementåˆ—è¡¨...")
        for i, condition in enumerate(initial_conditions):
            if ' is ' in condition:
                parts = condition.split(' is ', 1)
                if len(parts) == 2:
                    input_part = parts[0].strip()
                    output_part = parts[1].strip()
                    output_parsed = self.reasoning_engine.parse_output_entities(output_part)
                    
                    stmt_node = self.create_statement_node(
                        condition, input_part, output_part, output_parsed, -1
                    )
                    stmt_node.is_premise = True
                    stmt_node.is_correct = True
                    stmt_node.node_type = "premise"
                    
                    self.statement_list.append(stmt_node)
        
        print(f"âœ… åˆå§‹Statementåˆ—è¡¨æ„å»ºå®Œæˆï¼ŒåŒ…å« {len(self.statement_list)} ä¸ªå‰æèŠ‚ç‚¹")
        
        # éå†æ¯ä¸ªactualèŠ‚ç‚¹
        print(f"\nğŸ” å¼€å§‹å¤„ç†actualèŠ‚ç‚¹...")
        actual_nodes_processed = 0
        new_nodes_added = 0
        existing_nodes_updated = 0
        
        for i, node in enumerate(normalized_nodes):
            if node.get("type") == "actual":
                actual_nodes_processed += 1
                current_statement = node.get("original", f"{node['input']} is {node['output']}")
                
                print(f"\n   èŠ‚ç‚¹ {actual_nodes_processed}: {current_statement}")
                
                # æ­¥éª¤1: æ£€æŸ¥æ˜¯å¦å·²åœ¨statementåˆ—è¡¨ä¸­
                existing_node = self.find_statement_node(current_statement)
                if existing_node:
                    existing_node.occurrence_count += 1
                    existing_nodes_updated += 1
                    print(f"      â†» é‡å¤èŠ‚ç‚¹ï¼Œè®¡æ•°: {existing_node.occurrence_count}")
                    continue
                
                # æ­¥éª¤2: æ£€æŸ¥æ˜¯å¦åœ¨é¢˜ç›®æ¡ä»¶ä¸­
                if self.is_in_premises(current_statement, initial_conditions):
                    print(f"      ğŸ“‹ å‰ææ¡ä»¶ï¼ˆå·²å¤„ç†ï¼‰")
                    continue
                
                # æ­¥éª¤3: è¿™æ˜¯ä¸€ä¸ªæ–°çš„æ¨ç†èŠ‚ç‚¹ï¼Œéœ€è¦éªŒè¯
                print(f"      ğŸ” éªŒè¯æ–°èŠ‚ç‚¹...")
                
                # åˆ›å»ºç›®æ ‡èŠ‚ç‚¹ç”¨äºéªŒè¯
                target_node = {
                    "input": node['input'],
                    "output": node['output'],
                    "output_parsed": node['output_parsed'],
                    "original": current_statement,
                    "type": "target"
                }
                
                # å…ˆç”¨æ­£ç¡®çš„statementåšæ¡ä»¶éªŒè¯
                correct_premises = self.get_correct_statements_as_premises()
                is_provable_with_correct = self.reasoning_engine.is_provable(
                    target_node, correct_premises, debug=False
                )
                
                is_provable = is_provable_with_correct
                if not is_provable_with_correct:
                    # å¦‚æœç”¨æ­£ç¡®çš„æ¨ä¸å‡ºï¼Œå†åŠ ä¸Šé”™è¯¯çš„statementè¯•è¯•
                    all_premises = self.get_all_statements_as_premises()
                    is_provable = self.reasoning_engine.is_provable(
                        target_node, all_premises, debug=False
                    )
                
                # åˆ›å»ºæ–°çš„StatementèŠ‚ç‚¹
                stmt_node = self.create_statement_node(
                    current_statement, node['input'], node['output'], 
                    node['output_parsed'], i
                )
                
                if is_provable:
                    stmt_node.is_correct = True
                    stmt_node.node_type = "derived"
                    
                    # åœ¨LoGå›¾ä¸­æ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹å¹¶ç‚¹äº®
                    log_node = self.find_corresponding_log_node(current_statement)
                    if log_node:
                        self.illuminate_log_node(log_node)
                        print(f"      âœ… éªŒè¯æˆåŠŸ + LoGåŒ¹é…")
                    else:
                        print(f"      âœ… éªŒè¯æˆåŠŸ (LoGä¸­æ— å¯¹åº”èŠ‚ç‚¹)")
                else:
                    stmt_node.is_correct = False
                    stmt_node.node_type = "hallucination"
                    print(f"      âŒ éªŒè¯å¤±è´¥ - å¹»è§‰èŠ‚ç‚¹")
                
                # æ— è®ºæ­£ç¡®ä¸å¦éƒ½åŠ å…¥Statementåˆ—è¡¨
                self.statement_list.append(stmt_node)
                new_nodes_added += 1
        
        print(f"\nğŸ” åˆ†ææ¨ç†è·¯å¾„å®Œæ•´æ€§...")
        
        # å¯¹æ‰€æœ‰éå‰æèŠ‚ç‚¹åˆ†ææ¨ç†è·¯å¾„
        path_analysis_results = []
        perfect_reasoning_count = 0
        partial_reasoning_count = 0
        invalid_reasoning_count = 0
        
        for stmt_node in self.statement_list:
            if not stmt_node.is_premise:
                analysis_result = self.analyze_reasoning_path(stmt_node)
                path_analysis_results.append(analysis_result)
                
                if stmt_node.reasoning_quality == "perfect":
                    perfect_reasoning_count += 1
                elif stmt_node.reasoning_quality == "partial":
                    partial_reasoning_count += 1
                else:
                    invalid_reasoning_count += 1
        
        print(f"\n{'='*60}")
        print(f"[åå¤„ç†] å¤„ç†å®Œæˆ")
        print(f"{'='*60}")
        
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡:")
        print(f"   - å¤„ç†çš„actualèŠ‚ç‚¹: {actual_nodes_processed}")
        print(f"   - æ–°å¢èŠ‚ç‚¹: {new_nodes_added}")
        print(f"   - æ›´æ–°å·²å­˜åœ¨èŠ‚ç‚¹: {existing_nodes_updated}")
        print(f"   - ç‚¹äº®çš„LoGèŠ‚ç‚¹: {len(self.illuminated_nodes)}")
        
        print(f"\nğŸ¯ æ¨ç†è´¨é‡ç»Ÿè®¡:")
        print(f"   - å®Œç¾æ¨ç†: {perfect_reasoning_count} (èŠ‚ç‚¹æ­£ç¡® + è·¯å¾„æ­£ç¡®)")
        print(f"   - éƒ¨åˆ†æ¨ç†: {partial_reasoning_count} (èŠ‚ç‚¹æ­£ç¡® + è·¯å¾„éƒ¨åˆ†é”™è¯¯)")
        print(f"   - æ— æ•ˆæ¨ç†: {invalid_reasoning_count} (èŠ‚ç‚¹é”™è¯¯æˆ–è·¯å¾„å®Œå…¨é”™è¯¯)")
        
        # æ‰“å°æœ€ç»ˆçš„Statementåˆ—è¡¨
        self.print_statement_summary()
        
        # è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
        metrics = self.calculate_all_metrics()
        
        return {
            "total_nodes_processed": actual_nodes_processed,
            "new_nodes_added": new_nodes_added,
            "existing_nodes_updated": existing_nodes_updated,
            "statement_list_size": len(self.statement_list),
            "illuminated_log_nodes": len(self.illuminated_nodes),
            "correct_statements": len([s for s in self.statement_list if s.is_correct]),
            "incorrect_statements": len([s for s in self.statement_list if not s.is_correct]),
            "reasoning_quality": {
                "perfect_reasoning": perfect_reasoning_count,
                "partial_reasoning": partial_reasoning_count,
                "invalid_reasoning": invalid_reasoning_count
            },
            "path_analysis_results": path_analysis_results,
            "evaluation_metrics": metrics
        }
    
    def print_statement_summary(self):
        """æ‰“å°Statementåˆ—è¡¨æ‘˜è¦"""
        print(f"\nğŸ“Š === Statementåˆ—è¡¨åˆ†æ ===")
        
        premise_count = len([s for s in self.statement_list if s.is_premise])
        derived_count = len([s for s in self.statement_list if s.node_type == "derived"])
        hallucination_count = len([s for s in self.statement_list if s.node_type == "hallucination"])
        
        print(f"ğŸ“ˆ èŠ‚ç‚¹ç»Ÿè®¡:")
        print(f"   - æ€»èŠ‚ç‚¹æ•°: {len(self.statement_list)}")
        print(f"   - å‰æèŠ‚ç‚¹: {premise_count}")
        print(f"   - æ¨ç†èŠ‚ç‚¹: {derived_count}")
        print(f"   - å¹»è§‰èŠ‚ç‚¹: {hallucination_count}")
        print(f"   - ç‚¹äº®LoGèŠ‚ç‚¹: {len(self.illuminated_nodes)}")
        
        if hallucination_count > 0:
            print(f"\nâŒ å¹»è§‰èŠ‚ç‚¹è¯¦æƒ…:")
            for i, stmt_node in enumerate(self.statement_list):
                if stmt_node.node_type == "hallucination":
                    print(f"   - {stmt_node.original_statement} (å‡ºç°{stmt_node.occurrence_count}æ¬¡)")
        
        if derived_count > 0:
            print(f"\nâœ… æ¨ç†èŠ‚ç‚¹è¯¦æƒ…:")
            for i, stmt_node in enumerate(self.statement_list):
                if stmt_node.node_type == "derived":
                    quality_emoji = "ğŸŸ¢" if stmt_node.reasoning_quality == "perfect" else "ğŸŸ¡" if stmt_node.reasoning_quality == "partial" else "ğŸ”´"
                    print(f"   {quality_emoji} {stmt_node.original_statement} (å‡ºç°{stmt_node.occurrence_count}æ¬¡, è´¨é‡:{stmt_node.reasoning_quality})")
                    
                    if stmt_node.invalid_dependencies:
                        print(f"      âš ï¸  æ— æ•ˆä¾èµ–: {stmt_node.invalid_dependencies}")
        
        if len(self.illuminated_nodes) > 0:
            print(f"\nğŸ”¥ ç‚¹äº®çš„LoGèŠ‚ç‚¹:")
            for node_id in self.illuminated_nodes:
                print(f"   - {node_id}")
        else:
            print(f"\nâš ï¸  æœªç‚¹äº®ä»»ä½•LoGèŠ‚ç‚¹")
    
    def calculate_coverage_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—CoverageæŒ‡æ ‡ï¼ˆç±»ä¼¼å¬å›ç‡ï¼‰
        
        Returns:
            CoverageæŒ‡æ ‡ç»“æœ
        """
        print(f"\nğŸ¯ è®¡ç®—CoverageæŒ‡æ ‡...")
        
        # 1.1 æ·±åº¦Coverage - æ¨å‡ºå­æ ‘å¯¹åº”æ ‡å‡†LoGå›¾çš„æœ€å¤§hop
        max_depth_reached = 0
        deepest_illuminated_node = None
        
        for node_id in self.illuminated_nodes:
            # åœ¨LoGå›¾ä¸­æ‰¾åˆ°å¯¹åº”èŠ‚ç‚¹çš„æ·±åº¦
            for log_node in self.log_graph:
                if log_node.get('output', '') == node_id:
                    depth = log_node.get('depth', 0)
                    if depth > max_depth_reached:
                        max_depth_reached = depth
                        deepest_illuminated_node = node_id
                    break
        
        # è®¡ç®—LoGå›¾çš„æœ€å¤§æ·±åº¦
        max_log_depth = max([node.get('depth', 0) for node in self.log_graph]) if self.log_graph else 0
        depth_coverage_ratio = max_depth_reached / max_log_depth if max_log_depth > 0 else 0
        
        print(f"   æ·±åº¦Coverage: {max_depth_reached}/{max_log_depth} = {depth_coverage_ratio:.2%}")
        if deepest_illuminated_node:
            print(f"   æœ€æ·±ç‚¹äº®èŠ‚ç‚¹: {deepest_illuminated_node} (æ·±åº¦{max_depth_reached})")
        
        # 1.2 èŠ‚ç‚¹Coverage - æ ‡å‡†LoGè¢«ç‚¹äº®çš„èŠ‚ç‚¹æ¯”ä¾‹
        total_log_nodes = len(self.log_graph)
        illuminated_count = len(self.illuminated_nodes)
        node_coverage_ratio = illuminated_count / total_log_nodes if total_log_nodes > 0 else 0
        
        print(f"   èŠ‚ç‚¹Coverage: {illuminated_count}/{total_log_nodes} = {node_coverage_ratio:.2%}")
        
        # 1.3 æ¯ä¸ªæ·±åº¦çš„èŠ‚ç‚¹ç‚¹äº®æ¯”ä¾‹
        depth_stats = {}
        for log_node in self.log_graph:
            depth = log_node.get('depth', 0)
            node_output = log_node.get('output', '')
            
            if depth not in depth_stats:
                depth_stats[depth] = {'total': 0, 'illuminated': 0}
            
            depth_stats[depth]['total'] += 1
            if node_output in self.illuminated_nodes:
                depth_stats[depth]['illuminated'] += 1
        
        print(f"   å„æ·±åº¦ç‚¹äº®æ¯”ä¾‹:")
        for depth in sorted(depth_stats.keys()):
            stats = depth_stats[depth]
            ratio = stats['illuminated'] / stats['total'] if stats['total'] > 0 else 0
            print(f"     æ·±åº¦{depth}: {stats['illuminated']}/{stats['total']} = {ratio:.2%}")
        
        return {
            "depth_coverage": {
                "max_depth_reached": max_depth_reached,
                "max_log_depth": max_log_depth,
                "ratio": depth_coverage_ratio,
                "deepest_node": deepest_illuminated_node
            },
            "node_coverage": {
                "illuminated_count": illuminated_count,
                "total_log_nodes": total_log_nodes,
                "ratio": node_coverage_ratio
            },
            "depth_distribution": depth_stats
        }
    
    def calculate_precision_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—PrecisionæŒ‡æ ‡
        
        Returns:
            PrecisionæŒ‡æ ‡ç»“æœ
        """
        print(f"\nğŸ¯ è®¡ç®—PrecisionæŒ‡æ ‡...")
        
        # åªè€ƒè™‘éå‰æèŠ‚ç‚¹
        non_premise_nodes = [s for s in self.statement_list if not s.is_premise]
        total_derived_nodes = len(non_premise_nodes)
        
        if total_derived_nodes == 0:
            print(f"   æ²¡æœ‰æ¨ç†èŠ‚ç‚¹å¯ä¾›åˆ†æ")
            return {
                "error_rate": {"provable_count": 0, "total_count": 0, "ratio": 0},
                "strict_error_rate": {"valid_count": 0, "total_count": 0, "ratio": 0}
            }
        
        # 2.1 Error Rate - æ¨å‡ºèŠ‚ç‚¹æœ‰å¤šå°‘æ˜¯is_provableçš„
        provable_count = len([s for s in non_premise_nodes if s.is_correct])
        error_rate = 1 - (provable_count / total_derived_nodes)
        
        print(f"   Error Rate: {total_derived_nodes - provable_count}/{total_derived_nodes} = {error_rate:.2%}")
        print(f"   å¯æ¨å¯¼èŠ‚ç‚¹: {provable_count}/{total_derived_nodes}")
        
        # 2.2 Strict Error Rate - èŠ‚ç‚¹is_provableä¸”æ‰€æœ‰ç¥–å…ˆéƒ½æ­£ç¡®
        strict_valid_count = 0
        
        for stmt_node in non_premise_nodes:
            if stmt_node.is_correct:  # èŠ‚ç‚¹æœ¬èº«å¯æ¨å¯¼
                # æ£€æŸ¥æ‰€æœ‰ä¾èµ–èŠ‚ç‚¹æ˜¯å¦éƒ½æ­£ç¡®
                all_deps_valid = True
                for dep_statement in stmt_node.dependency_nodes:
                    dep_node = self.find_statement_node(dep_statement)
                    if dep_node and not dep_node.is_correct:
                        all_deps_valid = False
                        break
                
                if all_deps_valid:
                    strict_valid_count += 1
        
        strict_error_rate = 1 - (strict_valid_count / total_derived_nodes)
        
        print(f"   Strict Error Rate: {total_derived_nodes - strict_valid_count}/{total_derived_nodes} = {strict_error_rate:.2%}")
        print(f"   ä¸¥æ ¼æœ‰æ•ˆèŠ‚ç‚¹: {strict_valid_count}/{total_derived_nodes}")
        
        # è¯¦ç»†åˆ†æ
        print(f"\n   è¯¦ç»†åˆ†æ:")
        perfect_count = len([s for s in non_premise_nodes if s.reasoning_quality == "perfect"])
        partial_count = len([s for s in non_premise_nodes if s.reasoning_quality == "partial"])
        invalid_count = len([s for s in non_premise_nodes if s.reasoning_quality == "invalid"])
        
        print(f"     å®Œç¾æ¨ç†: {perfect_count} ({perfect_count/total_derived_nodes:.2%})")
        print(f"     éƒ¨åˆ†æ¨ç†: {partial_count} ({partial_count/total_derived_nodes:.2%})")
        print(f"     æ— æ•ˆæ¨ç†: {invalid_count} ({invalid_count/total_derived_nodes:.2%})")
        
        return {
            "error_rate": {
                "provable_count": provable_count,
                "total_count": total_derived_nodes,
                "ratio": error_rate
            },
            "strict_error_rate": {
                "valid_count": strict_valid_count,
                "total_count": total_derived_nodes,
                "ratio": strict_error_rate
            },
            "quality_distribution": {
                "perfect": perfect_count,
                "partial": partial_count,
                "invalid": invalid_count,
                "total": total_derived_nodes
            }
        }
    
    def calculate_all_metrics(self) -> Dict[str, Any]:
        """
        è®¡ç®—æ‰€æœ‰æŒ‡æ ‡
        
        Returns:
            å®Œæ•´çš„æŒ‡æ ‡ç»“æœ
        """
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
        print(f"{'='*60}")
        
        coverage_metrics = self.calculate_coverage_metrics()
        precision_metrics = self.calculate_precision_metrics()
        
        return {
            "coverage": coverage_metrics,
            "precision": precision_metrics,
            "summary": {
                "total_statements": len(self.statement_list),
                "premise_statements": len([s for s in self.statement_list if s.is_premise]),
                "derived_statements": len([s for s in self.statement_list if not s.is_premise]),
                "illuminated_log_nodes": len(self.illuminated_nodes),
                "total_log_nodes": len(self.log_graph)
            }
        }


class StepByStepEvaluator2:
    def __init__(self, api_key: str, model_name: str = "deepseek-reasoner", 
                 api_base: str = "https://api.deepseek.com/beta", debug_mode: bool = False,
                 llm_debug_mode: bool = False, api_mode: str = "commercial"):
        """
        åˆå§‹åŒ–é€æ­¥è¯„ä¼°å™¨2.0
        
        Args:
            api_key: APIå¯†é’¥
            model_name: æ¨¡å‹åç§°
            api_base: APIåŸºç¡€URL
            debug_mode: è°ƒè¯•æ¨¡å¼ï¼ˆè·³è¿‡æ‰€æœ‰APIè°ƒç”¨ï¼‰
            llm_debug_mode: LLMè°ƒè¯•æ¨¡å¼ï¼ˆåªåšæå–å’Œè®°å½•ï¼‰
            api_mode: APIæ¨¡å¼ï¼Œ"commercial"æˆ–"vllm"
        """
        self.debug_mode = debug_mode
        self.llm_debug_mode = llm_debug_mode
        self.model_name = model_name
        self.api_mode = api_mode
        
        if not debug_mode:
            # åªåœ¨éè°ƒè¯•æ¨¡å¼ä¸‹å¯¼å…¥å’Œåˆå§‹åŒ–APIå®¢æˆ·ç«¯
            from apply_llm import DeepSeekAPIClient
            self.client = DeepSeekAPIClient(
                api_key=api_key,
                model_name=model_name,
                api_base=api_base,
                max_new_tokens=5000
            )
        else:
            self.client = None
            
        self.extract_prompt_template = self.load_extract_prompt()
        self.statement_processor = StatementProcessor()
        self.reasoning_engine = LogicalReasoningEngine(max_depth=1000, timeout=600)
        self.post_processor = PostProcessor(self.reasoning_engine)
        
        # åˆ›å»ºLLMæå–ç»“æœç¼“å­˜ç›®å½•
        self.cache_dir = "./LLM_extract_node"
        os.makedirs(self.cache_dir, exist_ok=True)
    
    def load_extract_prompt(self) -> str:
        """åŠ è½½æå–æç¤ºæ¨¡æ¿"""
        try:
            with open('extract_prompt_2.txt', 'r', encoding='utf-8') as f:
                return f.read().strip()
        except FileNotFoundError:
            raise FileNotFoundError("æ‰¾ä¸åˆ° extract_prompt_2.txt æ–‡ä»¶")
    
    def load_evaluation_log(self, log_path: str) -> Dict[str, Any]:
        """åŠ è½½è¯„ä¼°æ—¥å¿—æ–‡ä»¶"""
        try:
            with open(log_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ—¥å¿—æ–‡ä»¶: {log_path}")
        except json.JSONDecodeError as e:
            raise ValueError(f"JSONè§£æé”™è¯¯: {e}")
    
    def load_log_graph_data(self, input_file_path: str) -> Dict[int, List[Dict[str, Any]]]:
        """
        ä»LoGæ•°æ®æ–‡ä»¶ä¸­åŠ è½½å›¾æ•°æ®
        
        Args:
            input_file_path: LoGæ•°æ®æ–‡ä»¶è·¯å¾„ (å¦‚ ./generated_data/LoG_5.jsonl)
            
        Returns:
            å­—å…¸ï¼Œkeyä¸ºexampleçš„idï¼Œvalueä¸ºå¯¹åº”çš„graphæ•°æ®
        """
        graph_data = {}
        try:
            with open(input_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        example = json.loads(line)
                        example_id = example.get('id', -1)
                        graph = example.get('graph', [])
                        graph_data[example_id] = graph
            
            print(f"ä» {input_file_path} åŠ è½½äº† {len(graph_data)} ä¸ªä¾‹å­çš„å›¾æ•°æ®")
            return graph_data
            
        except FileNotFoundError:
            print(f"è­¦å‘Š: æ‰¾ä¸åˆ°LoGæ•°æ®æ–‡ä»¶: {input_file_path}")
            return {}
        except Exception as e:
            print(f"åŠ è½½LoGå›¾æ•°æ®æ—¶å‡ºé”™: {e}")
            return {}
    
    def get_cache_file_path(self, log_path: str, record_index: int) -> str:
        """
        è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„
        
        Args:
            log_path: åŸå§‹æ—¥å¿—æ–‡ä»¶è·¯å¾„
            record_index: è®°å½•ç´¢å¼•
            
        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        import hashlib
        
        # åŸºäºæ—¥å¿—æ–‡ä»¶è·¯å¾„å’Œè®°å½•ç´¢å¼•ç”Ÿæˆç¼“å­˜æ–‡ä»¶å
        log_file_name = os.path.basename(log_path).replace('.json', '')
        cache_file_name = f"{log_file_name}_record_{record_index}.json"
        return os.path.join(self.cache_dir, cache_file_name)
    
    def load_cached_extraction(self, cache_path: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½ç¼“å­˜çš„æå–ç»“æœ
        
        Args:
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            
        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                print(f"ğŸ“ ä»ç¼“å­˜åŠ è½½æå–ç»“æœ: {os.path.basename(cache_path)}")
                return cached_data
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def save_extraction_to_cache(self, cache_path: str, extraction_data: Dict[str, Any]):
        """
        ä¿å­˜æå–ç»“æœåˆ°ç¼“å­˜
        
        Args:
            cache_path: ç¼“å­˜æ–‡ä»¶è·¯å¾„
            extraction_data: æå–æ•°æ®
        """
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(extraction_data, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ ä¿å­˜æå–ç»“æœåˆ°ç¼“å­˜: {os.path.basename(cache_path)}")
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")
    
    def split_reasoning_text(self, text: str) -> tuple[List[str], List[str]]:
        """
        å°†æ¨ç†æ–‡æœ¬æŒ‰å¥å­åˆ†å‰²ï¼ŒåŒæ—¶ä¿å­˜åˆ†éš”ç¬¦ä¿¡æ¯
        
        Args:
            text: æ¨ç†æ–‡æœ¬
            
        Returns:
            (å¥å­åˆ—è¡¨, åˆ†éš”ç¬¦åˆ—è¡¨)
        """
        # ä½¿ç”¨å¤šä¸ªåˆ†éš”ç¬¦è¿›è¡Œåˆ†å‰²ï¼šå¥å·ã€é—®å·ã€æ„Ÿå¹å·ã€è¿ç»­æ¢è¡Œç¬¦
        parts = re.split(r'([.!?ã€‚ï¼Ÿï¼]|\n\n+)', text)
        
        sentences = []
        separators = []
        
        i = 0
        while i < len(parts):
            if i + 1 < len(parts):
                content = parts[i].strip()
                separator = parts[i + 1]
                
                if content:  # åªå¤„ç†æœ‰å†…å®¹çš„å¥å­
                    if re.match(r'\n\n+', separator):
                        sentences.append(content)
                        separators.append('\n\n')
                    elif separator in '.!?ã€‚ï¼Ÿï¼':
                        sentences.append(content + separator)
                        separators.append(' ')
                i += 2
            else:
                sentence = parts[i].strip()
                if sentence:
                    sentences.append(sentence)
                    separators.append('')
                i += 1
        
        return sentences, separators
    
    
    def create_analysis_prompt(self, current_sentence: str) -> str:
        """
        åˆ›å»ºåˆ†ææç¤ºï¼Œå°†å½“å‰å¥å­æ’å…¥åˆ°æå–æç¤ºæ¨¡æ¿ä¸­
        
        Args:
            current_sentence: å½“å‰å¥å­
            
        Returns:
            å®Œæ•´çš„åˆ†ææç¤º
        """
        # ä½¿ç”¨æ¨¡æ¿æ›¿æ¢
        template = self.extract_prompt_template
        prompt = template.replace("{current_sentence}", current_sentence)
        
        return prompt
    
    def extract_json_from_response(self, response_text: str) -> Dict[str, Any]:
        """
        ä»å“åº”ä¸­æå–JSONç»“æœ
        
        Args:
            response_text: æ¨¡å‹å“åº”æ–‡æœ¬
            
        Returns:
            æå–çš„JSONå¯¹è±¡ï¼Œå¦‚æœæå–å¤±è´¥è¿”å›é”™è¯¯ä¿¡æ¯
        """
        try:
            # å°è¯•ç›´æ¥è§£ææ•´ä¸ªå“åº”ä¸ºJSON
            return json.loads(response_text.strip())
        except json.JSONDecodeError:
            # å°è¯•ä»ä»£ç å—ä¸­æå–JSON
            json_pattern = r'```json\s*(\{.*?\})\s*```'
            match = re.search(json_pattern, response_text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(1))
                except json.JSONDecodeError:
                    pass
            
            # å°è¯•æå–ä»»ä½•JSONå¯¹è±¡
            json_pattern = r'\{[^}]*"statements"[^}]*\}'
            matches = re.findall(json_pattern, response_text, re.DOTALL)
            
            for match in matches:
                try:
                    return json.loads(match)
                except json.JSONDecodeError:
                    continue
            
            # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return {
                "error": "æ— æ³•ä»å“åº”ä¸­æå–æœ‰æ•ˆçš„JSON",
                "raw_response": response_text
            }
    
    def extract_statements_from_sentence(self, current_sentence: str, sentence_index: int, total_sentences: int) -> Dict[str, Any]:
        """
        ä»å•ä¸ªå¥å­ä¸­æå–statements
        
        Args:
            current_sentence: å½“å‰å¥å­
            sentence_index: å¥å­ç´¢å¼•
            total_sentences: æ€»å¥å­æ•°
            
        Returns:
            æå–ç»“æœï¼ŒåŒ…å«åŸå¥å­ã€æç¤ºã€å“åº”å’Œæå–çš„statements
        """
        print(f"[{sentence_index+1}/{total_sentences}] è¾“å…¥: {current_sentence[:100]}...")
        
        # åˆ›å»ºåˆ†ææç¤ºï¼ˆåªåŒ…å«å½“å‰å¥å­ï¼‰
        prompt = self.create_analysis_prompt(current_sentence)
        
        try:
            if self.debug_mode:
                # å®Œå…¨è°ƒè¯•æ¨¡å¼ï¼šä¸è°ƒç”¨API
                response_text = "[DEBUG MODE - NO API CALL]"
                thinking = ""
                json_result = {"statements": ["debug statement 1", "debug statement 2"]}
                print(f"ç»“æœ: è°ƒè¯•æ¨¡å¼ - {len(json_result.get('statements', []))} ä¸ªstatements")
            else:
                # è°ƒç”¨API
                response = self.client.get_response(prompt, temperature=0.0)
                
                # æå–å“åº”æ–‡æœ¬
                if self.api_mode == "commercial":
                    response_text = response['choices'][0]['message']['content']
                    thinking = response['choices'][0]['message'].get('reasoning_content', '')
                elif self.api_mode == "vllm":
                    response_text = response['choices'][0]['text'].strip()
                    thinking = ''
                else:
                    # é»˜è®¤ä½¿ç”¨commercialæ¨¡å¼
                    response_text = response['choices'][0]['message']['content']
                    thinking = response['choices'][0]['message'].get('reasoning_content', '')
                
                # æå–JSONç»“æœ
                json_result = self.extract_json_from_response(response_text)
                
                # æ‰“å°åˆ†æç»“æœ
                if "error" not in json_result:
                    statements = json_result.get('statements', [])
                    print(f"ç»“æœ: æå–åˆ° {len(statements)} ä¸ªstatements")
                    if statements:
                        for i, stmt in enumerate(statements):
                            if isinstance(stmt, dict):
                                stmt_type = stmt.get('type', 'unknown')
                                stmt_text = stmt.get('statement', str(stmt))
                                print(f"  {i+1}. [{stmt_type}] {stmt_text}")
                            else:
                                print(f"  {i+1}. [legacy] {stmt}")
                else:
                    print(f"æå–å¤±è´¥: {json_result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            
            return {
                "sentence": current_sentence,
                "sentence_index": sentence_index,
                "prompt": prompt,
                "response_text": response_text,
                "thinking": thinking,
                "json_result": json_result,
                "success": "error" not in json_result
            }
            
        except Exception as e:
            print(f"æå–statementsæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return {
                "sentence": current_sentence,
                "sentence_index": sentence_index,
                "prompt": prompt,
                "error": str(e),
                "success": False
            }
    
    def evaluate_reasoning_process(self, log_path: str, output_path: str = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨ç†è¿‡ç¨‹
        
        Args:
            log_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            è¯„ä¼°ç»“æœ
        """
        print(f"å¼€å§‹è¯„ä¼°æ¨ç†è¿‡ç¨‹ï¼ˆç‰ˆæœ¬2.0ï¼‰...")
        print(f"æ—¥å¿—æ–‡ä»¶: {log_path}")
        print(f"LLMè°ƒè¯•æ¨¡å¼: {self.llm_debug_mode}")
        
        # åŠ è½½æ—¥å¿—
        log_data = self.load_evaluation_log(log_path)
        details = log_data.get('details', [])
        
        if not details:
            raise ValueError("æ—¥å¿—æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°detailsæ•°æ®")
        
        print(f"æ‰¾åˆ° {len(details)} æ¡è®°å½•")
        
        # å°è¯•ä»input_fileå­—æ®µè·å–å¯¹åº”çš„LoGæ•°æ®æ–‡ä»¶è·¯å¾„
        input_file = log_data.get('input_file', '')
        graph_data_dict = {}
        
        if input_file:
            print(f"æ£€æµ‹åˆ°input_file: {input_file}")
            graph_data_dict = self.load_log_graph_data(input_file)
        else:
            print("æœªæ‰¾åˆ°input_fileå­—æ®µï¼Œå°è¯•æ¨æ–­LoGæ•°æ®æ–‡ä»¶è·¯å¾„")
            # ä»log_pathæ¨æ–­å¯¹åº”çš„LoGæ•°æ®æ–‡ä»¶
            if 'LoG_5' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_5.jsonl')
            elif 'LoG_4' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_4.jsonl')
            elif 'LoG_6' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_6.jsonl')
            elif 'LoG_7' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_7.jsonl')
            elif 'LoG_8' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_8.jsonl')
            elif 'LoG_9' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_9.jsonl')
            elif 'LoG_10' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_10.jsonl')
            elif 'LoG_11' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_11.jsonl')
            elif 'LoG_12' in log_path:
                graph_data_dict = self.load_log_graph_data('./generated_data/LoG_12.jsonl')
            else:
                print(f"æœªçŸ¥çš„LoGæ–‡ä»¶ç±»å‹: {log_path}")
        
        # è¯„ä¼°ç»“æœ
        evaluation_results = {
            "log_path": log_path,
            "total_records": len(details),
            "model_name": self.model_name,
            "llm_debug_mode": self.llm_debug_mode,
            "processed_records": [],
            "summary": {
                "total_sentences": 0,
                "successful_extractions": 0,
                "failed_extractions": 0,
                "total_statements": 0
            }
        }
        
        # ä¸ºäº†æµ‹è¯•æ–¹ä¾¿ï¼Œåªå¤„ç†ç¬¬ä¸€æ¡è®°å½•
        print("\nå¤„ç†ç¬¬ä¸€æ¡è®°å½•ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰...")
        
        record = details[0]
        record_index = record.get('index', 0)
        print(f"è®°å½•ç´¢å¼•: {record_index}")
        print(f"é—®é¢˜çŠ¶æ€: {record.get('status', 'N/A')}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æå–ç»“æœ
        cache_path = self.get_cache_file_path(log_path, record_index)
        cached_result = self.load_cached_extraction(cache_path)
        
        if cached_result:
            # ä½¿ç”¨ç¼“å­˜çš„ç»“æœ
            sentence_extractions = cached_result.get('sentence_extractions', [])
            all_statements = cached_result.get('all_statements', [])
            sentences = cached_result.get('sentences', [])
            initial_conditions = cached_result.get('initial_conditions', [])
            reasoning_text = cached_result.get('reasoning_text', '')
            thinking_text = cached_result.get('thinking_text', '')
            
            print(f"âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ:")
            print(f"   - å¥å­æ•°: {len(sentences)}")
            print(f"   - æå–çš„è¯­å¥æ•°: {len(all_statements)}")
            print(f"   - åˆå§‹æ¡ä»¶æ•°: {len(initial_conditions)}")
            
        else:
            # æ‰§è¡ŒLLMæå–
            print(f"ğŸ”„ æ‰§è¡ŒLLMæå–ï¼ˆæœªæ‰¾åˆ°ç¼“å­˜ï¼‰...")
            
            # æå–åˆå§‹æ¡ä»¶
            original_question = record.get('original_question', '')
            initial_conditions = self.statement_processor.extract_initial_conditions(original_question)
            print(f"æå–åˆ° {len(initial_conditions)} ä¸ªåˆå§‹æ¡ä»¶:")
            for i, condition in enumerate(initial_conditions):
                print(f"  {i+1}. {condition}")
            
            # è·å–æ¨ç†è¿‡ç¨‹æ–‡æœ¬
            reasoning_text = record.get('full_response', '')
            thinking_text = record.get('thinking', '')

            if len(thinking_text) > 0:
                reasoning_text = thinking_text
            
            if not reasoning_text:
                print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ¨ç†è¿‡ç¨‹æ–‡æœ¬")
                return evaluation_results
            
            print(f"æ¨ç†æ–‡æœ¬é•¿åº¦: {len(reasoning_text)} å­—ç¬¦")
            
            # åˆ†å‰²å¥å­
            sentences, separators = self.split_reasoning_text(reasoning_text)
            print(f"åˆ†å‰²å¾—åˆ° {len(sentences)} ä¸ªå¥å­")
            
            # æå–æ¯ä¸ªå¥å­çš„statements
            sentence_extractions = []
            all_statements = []
            
            for i, sentence in enumerate(sentences):
                print(f"\n--- å¤„ç†å¥å­ {i+1}/{len(sentences)} ---")
                
                extraction = self.extract_statements_from_sentence(sentence, i, len(sentences))
                sentence_extractions.append(extraction)
                
                # æ”¶é›†æ‰€æœ‰statements
                if extraction["success"]:
                    statements = extraction["json_result"].get("statements", [])
                    # å¤„ç†æ–°æ ¼å¼çš„statementsï¼ˆå¸¦typeå­—æ®µï¼‰
                    for stmt in statements:
                        if isinstance(stmt, dict):
                            all_statements.append(stmt)
                        else:
                            # å…¼å®¹æ—§æ ¼å¼
                            all_statements.append({"type": "legacy", "statement": stmt})
                    
                    # æ›´æ–°ç»Ÿè®¡
                    evaluation_results["summary"]["successful_extractions"] += 1
                    evaluation_results["summary"]["total_statements"] += len(statements)
                else:
                    evaluation_results["summary"]["failed_extractions"] += 1
                
                evaluation_results["summary"]["total_sentences"] += 1
            
            # ä¿å­˜æå–ç»“æœåˆ°ç¼“å­˜
            extraction_cache_data = {
                "record_index": record_index,
                "initial_conditions": initial_conditions,
                "reasoning_text": reasoning_text,
                "thinking_text": thinking_text,
                "sentences": sentences,
                "sentence_extractions": sentence_extractions,
                "all_statements": all_statements,
                "extraction_timestamp": time.time()
            }
            
            self.save_extraction_to_cache(cache_path, extraction_cache_data)
        
        print(f"\n=== Statementæå–å®Œæˆ ===")
        print(f"æ€»å¥å­æ•°: {evaluation_results['summary']['total_sentences']}")
        print(f"æˆåŠŸæå–: {evaluation_results['summary']['successful_extractions']}")
        print(f"å¤±è´¥æå–: {evaluation_results['summary']['failed_extractions']}")
        print(f"æ€»statementsæ•°: {evaluation_results['summary']['total_statements']}")
        
        # æ¸…æ´—statementsæ ¼å¼
        print(f"\n=== å¼€å§‹æ¸…æ´—Statementæ ¼å¼ ===")
        original_count = len(all_statements)
        cleaned_statements = self.statement_processor.clean_statements_list(all_statements)
        cleaned_count = len(cleaned_statements)
        
        print(f"åŸå§‹statementsæ•°: {original_count}")
        print(f"æ¸…æ´—åstatementsæ•°: {cleaned_count}")
        print(f"è¿‡æ»¤æ‰çš„statementsæ•°: {original_count - cleaned_count}")
        
        if cleaned_count != original_count:
            print("æ¸…æ´—åçš„statements:")
            for i, stmt in enumerate(cleaned_statements):
                stmt_type = stmt.get('type', 'unknown')
                stmt_text = stmt.get('statement', '')
                print(f"  {i+1}. [{stmt_type}] {stmt_text}")
        
        # æ›´æ–°all_statementsä¸ºæ¸…æ´—åçš„ç‰ˆæœ¬
        all_statements = cleaned_statements
        
        # æ ‡å‡†åŒ–å¹¶è§£æä¸ºèŠ‚ç‚¹æ ¼å¼
        print(f"\n=== å¼€å§‹æ ‡å‡†åŒ–å’Œè§£æèŠ‚ç‚¹ ===")
        normalized_nodes = self.statement_processor.normalize_and_parse_statements(all_statements)
        normalized_count = len(normalized_nodes)
        
        print(f"æ¸…æ´—åstatementsæ•°: {cleaned_count}")
        print(f"æ ‡å‡†åŒ–åèŠ‚ç‚¹æ•°: {normalized_count}")
        print(f"è¿‡æ»¤æ‰çš„æ— æ•ˆå®ä½“æ•°: {cleaned_count - normalized_count}")
        
        if normalized_count > 0:
            print("æ ‡å‡†åŒ–åçš„èŠ‚ç‚¹:")
            for i, node in enumerate(normalized_nodes):
                node_type = node.get('type', 'unknown')
                original = node.get('original', '')
                input_part = node.get('input', '')
                output_part = node.get('output', '')
                print(f"  {i+1}. [{node_type}] {input_part} â†’ {output_part} (åŸå§‹: {original})")
        
        # æ‰§è¡Œæ–°çš„åå¤„ç†é€»è¾‘
        print(f"\nå¼€å§‹æ–°çš„åå¤„ç†...")
        
        # ä»è®°å½•ä¸­æå–LoGå›¾æ•°æ®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä»graph_data_dictä¸­è·å–
        graph_data = record.get('graph', [])
        if not graph_data and graph_data_dict:
            # å°è¯•ä»graph_data_dictä¸­è·å–å¯¹åº”çš„å›¾æ•°æ®
            record_index = record.get('index', -1)
            if record_index in graph_data_dict:
                graph_data = graph_data_dict[record_index]
                print(f"[åå¤„ç†] ä»LoGæ•°æ®æ–‡ä»¶ä¸­è·å–åˆ°å›¾æ•°æ®ï¼ŒèŠ‚ç‚¹æ•°: {len(graph_data)}")
        
        if graph_data:
            self.post_processor.load_log_graph(graph_data)
        else:
            print("[åå¤„ç†] è­¦å‘Š: æœªæ‰¾åˆ°LoGå›¾æ•°æ®")
        
        # æ‰§è¡Œåå¤„ç†
        post_processing_result = self.post_processor.process_nodes(
            normalized_nodes, initial_conditions
        )
        
        # ä¸ºäº†å‘åå…¼å®¹ï¼Œä¹Ÿä¿ç•™æ—§çš„åå¤„ç†ç»“æœ
        if self.llm_debug_mode:
            print(f"LLMè°ƒè¯•æ¨¡å¼ï¼šä½¿ç”¨ç®€åŒ–çš„æ—§åå¤„ç†ä½œä¸ºè¡¥å……")
            legacy_result = self.statement_processor.process_statements(all_statements, debug_mode=True)
        else:
            self.statement_processor.condition_list = initial_conditions
            legacy_result = self.statement_processor.process_statements(all_statements)
        
        post_processing_result["legacy_result"] = legacy_result
        
        # ä¿å­˜å¤„ç†çš„è®°å½•
        processed_record = {
            "original_record": {
                "index": record.get('index'),
                "status": record.get('status'),
                "question": record.get('extracted_question', ''),
                "expected": record.get('expected'),
                "predicted": record.get('predicted')
            },
            "initial_conditions": initial_conditions,
            "reasoning_text": reasoning_text,
            "thinking_text": thinking_text,
            "sentences": sentences,
            "sentence_extractions": sentence_extractions,
            "all_statements": all_statements,
            "cleaned_statements": cleaned_statements,
            "normalized_nodes": normalized_nodes,
            "post_processing_result": post_processing_result,
            "statement_list": [
                {
                    "original_statement": stmt.original_statement,
                    "input_entity": stmt.input_entity,
                    "output_entity": stmt.output_entity,
                    "occurrence_count": stmt.occurrence_count,
                    "is_correct": stmt.is_correct,
                    "is_premise": stmt.is_premise,
                    "node_type": stmt.node_type,
                    "first_occurrence_index": stmt.first_occurrence_index,
                    "path_is_valid": stmt.path_is_valid,
                    "reasoning_quality": stmt.reasoning_quality,
                    "dependency_nodes": stmt.dependency_nodes,
                    "invalid_dependencies": stmt.invalid_dependencies
                }
                for stmt in self.post_processor.statement_list
            ],
            "illuminated_log_nodes": list(self.post_processor.illuminated_nodes),
            "evaluation_metrics": post_processing_result.get("evaluation_metrics", {})
        }
        
        evaluation_results["processed_records"].append(processed_record)
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            base_name = os.path.splitext(os.path.basename(log_path))[0]
            output_path = f"step_by_step_evaluation_2_{base_name}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return evaluation_results
    
    def verify_reasoning_with_engine(self, initial_conditions: List[str], 
                                   normalized_nodes: List[Dict[str, Any]], 
                                   target_question: str = None, debug: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨æ¨ç†å¼•æ“éªŒè¯æ¨ç†è¿‡ç¨‹
        
        Args:
            initial_conditions: åˆå§‹æ¡ä»¶åˆ—è¡¨
            normalized_nodes: æ ‡å‡†åŒ–çš„èŠ‚ç‚¹åˆ—è¡¨
            target_question: ç›®æ ‡é—®é¢˜ï¼ˆå¯é€‰ï¼‰
            debug: æ˜¯å¦å¼€å¯è°ƒè¯•æ¨¡å¼
            
        Returns:
            éªŒè¯ç»“æœ
        """
        print(f"\n=== å¼€å§‹æ¨ç†å¼•æ“éªŒè¯ ===")
        
        # å°†åˆå§‹æ¡ä»¶è½¬æ¢ä¸ºèŠ‚ç‚¹æ ¼å¼
        premise_nodes = []
        for condition in initial_conditions:
            if ' is ' in condition:
                parts = condition.split(' is ', 1)
                if len(parts) == 2:
                    input_part = parts[0].strip()
                    output_part = parts[1].strip()
                    output_parsed = self.reasoning_engine.parse_output_entities(output_part)
                    
                    premise_nodes.append({
                        "original": condition,
                        "input": input_part,
                        "output": output_part,
                        "output_parsed": output_parsed,
                        "type": "initial_condition"
                    })
        
        print(f"è½¬æ¢äº† {len(premise_nodes)} ä¸ªåˆå§‹æ¡ä»¶ä¸ºå‰æèŠ‚ç‚¹")
        
        # éªŒè¯æ¯ä¸ªæ¨ç†æ­¥éª¤
        verification_results = []
        successful_verifications = 0
        
        for i, node in enumerate(normalized_nodes):
            if node.get("type") == "actual":  # åªéªŒè¯actualç±»å‹çš„èŠ‚ç‚¹
                print(f"\néªŒè¯èŠ‚ç‚¹ {i+1}: {node['input']} is {node['output']}")
                
                # æ„é€ ç›®æ ‡èŠ‚ç‚¹
                target_node = {
                    "input": node["input"],
                    "output": node["output"],
                    "output_parsed": node["output_parsed"],
                    "original": node.get("original", f"{node['input']} is {node['output']}"),
                    "type": "target"
                }
                
                # ä½¿ç”¨æ¨ç†å¼•æ“éªŒè¯
                start_time = time.time()
                is_provable = self.reasoning_engine.is_provable(
                    target_node, premise_nodes, debug=debug
                )
                verification_time = time.time() - start_time
                
                result = {
                    "node_index": i,
                    "node": node,
                    "target": target_node,
                    "is_provable": is_provable,
                    "verification_time": verification_time,
                    "status": "success" if is_provable else "failed"
                }
                
                verification_results.append(result)
                
                if is_provable:
                    successful_verifications += 1
                    print(f"  âœ“ éªŒè¯æˆåŠŸ ({verification_time:.2f}s)")
                    
                    # å¦‚æœéªŒè¯æˆåŠŸï¼Œå°†æ­¤èŠ‚ç‚¹æ·»åŠ åˆ°å‰æä¸­ï¼Œä¾›åç»­éªŒè¯ä½¿ç”¨
                    premise_nodes.append(target_node)
                else:
                    print(f"  âœ— éªŒè¯å¤±è´¥ ({verification_time:.2f}s)")
        
        # å¦‚æœæœ‰ç›®æ ‡é—®é¢˜ï¼Œä¹ŸéªŒè¯ä¸€ä¸‹
        target_verification = None
        if target_question and ' is ' in target_question:
            print(f"\néªŒè¯æœ€ç»ˆç›®æ ‡: {target_question}")
            parts = target_question.split(' is ', 1)
            if len(parts) == 2:
                input_part = parts[0].strip()
                output_part = parts[1].strip()
                output_parsed = self.reasoning_engine.parse_output_entities(output_part)
                
                target_node = {
                    "input": input_part,
                    "output": output_part,
                    "output_parsed": output_parsed,
                    "original": target_question,
                    "type": "final_target"
                }
                
                start_time = time.time()
                is_provable = self.reasoning_engine.is_provable(
                    target_node, premise_nodes, debug=debug
                )
                verification_time = time.time() - start_time
                
                target_verification = {
                    "target_question": target_question,
                    "target": target_node,
                    "is_provable": is_provable,
                    "verification_time": verification_time,
                    "status": "success" if is_provable else "failed"
                }
                
                if is_provable:
                    print(f"  âœ“ æœ€ç»ˆç›®æ ‡éªŒè¯æˆåŠŸ ({verification_time:.2f}s)")
                else:
                    print(f"  âœ— æœ€ç»ˆç›®æ ‡éªŒè¯å¤±è´¥ ({verification_time:.2f}s)")
        
        # ç»Ÿè®¡ç»“æœ
        total_nodes = len([n for n in normalized_nodes if n.get("type") == "actual"])
        success_rate = successful_verifications / total_nodes * 100 if total_nodes > 0 else 0
        
        print(f"\n=== éªŒè¯ç»“æœç»Ÿè®¡ ===")
        print(f"æ€»èŠ‚ç‚¹æ•°: {total_nodes}")
        print(f"éªŒè¯æˆåŠŸ: {successful_verifications}")
        print(f"éªŒè¯å¤±è´¥: {total_nodes - successful_verifications}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        return {
            "premise_nodes_count": len(premise_nodes),
            "total_nodes": total_nodes,
            "successful_verifications": successful_verifications,
            "failed_verifications": total_nodes - successful_verifications,
            "success_rate": success_rate,
            "verification_results": verification_results,
            "target_verification": target_verification,
            "reasoning_engine_config": {
                "max_depth": self.reasoning_engine.max_depth,
                "timeout": self.reasoning_engine.timeout
            }
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="é€æ­¥è¯„ä¼°æ¨ç†è¿‡ç¨‹ v2.0")
    parser.add_argument("--log_path", type=str, required=True,
                       help="è¯„ä¼°æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_path", type=str, default=None,
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰")
    parser.add_argument("--api_key", type=str, default="sk-b56f448069294b79967b8c897aebcec3",
                       help="APIå¯†é’¥")
    parser.add_argument("--model_name", type=str, default="deepseek-reasoner",
                       help="æ¨¡å‹åç§°")
    parser.add_argument("--api_base", type=str, default="https://api.deepseek.com/beta",
                       help="APIåŸºç¡€URL")
    parser.add_argument("--debug_mode", action="store_true",
                       help="è°ƒè¯•æ¨¡å¼ï¼Œä¸è°ƒç”¨APIï¼Œåªæµ‹è¯•æ–‡æœ¬å¤„ç†åŠŸèƒ½")
    parser.add_argument("--llm_debug_mode", action="store_true",
                       help="LLMè°ƒè¯•æ¨¡å¼ï¼Œåªåšæå–å’Œè®°å½•ï¼Œè·³è¿‡åå¤„ç†é€»è¾‘")
    parser.add_argument("--api_mode", type=str, default="commercial",
                       choices=["commercial", "vllm"],
                       help="APIæ¨¡å¼ï¼šcommercialï¼ˆå•†ä¸šAPIï¼‰æˆ–vllmï¼ˆVLLM APIï¼‰")
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = StepByStepEvaluator2(
            api_key=args.api_key,
            model_name=args.model_name,
            api_base=args.api_base,
            debug_mode=args.debug_mode,
            llm_debug_mode=args.llm_debug_mode,
            api_mode=args.api_mode
        )
        
        # æ‰§è¡Œè¯„ä¼°
        results = evaluator.evaluate_reasoning_process(
            log_path=args.log_path,
            output_path=args.output_path
        )
        
        print("\nè¯„ä¼°æˆåŠŸå®Œæˆï¼")
        
    except Exception as e:
        print(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
